{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d13168",
   "metadata": {},
   "source": [
    "# Neural Network Class for handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "39aba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0178e",
   "metadata": {},
   "source": [
    "## Reading test and training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "5e6150f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data1():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "\n",
    "    tr_d, va_d, te_d = load_data1()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "806326e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, _, testing = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf4d30",
   "metadata": {},
   "source": [
    "# Creating a network class:\n",
    "\n",
    "input: object is a list. The number of items in the list represents the number of layers, where the first input is the amount of nodes in the input layer, where the items up to the final item are the the amount of nodes in each hidden layer, where finally we have the final item being the amount of nodes in the output layer.\n",
    "\n",
    "more formally, we have:\n",
    "\n",
    "object[0] := |nodes in the input layer|,\n",
    "object[1: len(object)] := |nodes in each hidden layer|,\n",
    "object[-1] := |nodes in the final layer|\n",
    "\n",
    "These biases and weights are initialised with a gaussian distribution of mean 0 and std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "27f97305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #return 1.0/(1.0+np.exp(-x))\n",
    "    return scipy.special.expit(x)\n",
    "\n",
    "\n",
    "class network1(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        self.number_of_layers = len(sizes)\n",
    "        \n",
    "        self.sizes = sizes\n",
    "        \n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        \n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        \n",
    "    def forward_prop(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(w.dot(a) + b)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.forward_prop(x)), y) for (x, y) in test_data]\n",
    "        \n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    #we note that it is difficult to quickly compute the gradient of this big function space.\n",
    "    #hence we can take a stochastic approach, by seperating the function into equal sizes of batches which we can train on\n",
    "    #assuming we have enough batches as a sample, we can say that approximatley the average value of the gradient of the cost function from each \n",
    "    #batch will be roughly equal to the average over the entire function space\n",
    "    \n",
    "\n",
    "    def stochastic_gradient_descent(self, training_data, epochs, batch_size, eta, test_data = None):\n",
    "        \n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        n = len(training_data)\n",
    "        \n",
    "        #performing calcs for the number of epochs we assign (assumes the data is already randomly shuffled)\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            \n",
    "            #randomly shuffle the input data\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            #creating n batches of batch size: batch_size\n",
    "            training_batches = [training_data[k : k + batch_size] for k in range(0, n, batch_size)]\n",
    "            \n",
    "            for batch in training_batches:\n",
    "                #applying a step of gradient descent to each batch\n",
    "                self.update_batch(batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                 print(\"Epoch {0}: {1} / {2}\".format(i, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(i))\n",
    "                \n",
    "            \n",
    "            \n",
    "    def update_batch(self, batch, eta):\n",
    "        \n",
    "        #pd of cost function wrt bias\n",
    "        n_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        #pd of cost function wrt weight\n",
    "        n_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "\n",
    "        for x,y in batch:\n",
    "            \n",
    "            dCdB, dCdW = self.back_propagation(x,y)\n",
    "\n",
    "            n_b = [nb + d_nb for nb, d_nb in zip(n_b, dCdB)]\n",
    "            n_w = [nw + d_nw for nw, d_nw in zip(n_w, dCdW)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(batch))*nw for w, nw in zip(self.weights, n_w)]\n",
    "        \n",
    "        self.biases = [b-(eta/len(batch))*nb for b, nb in zip(self.biases, n_b)]\n",
    "    \n",
    "    \n",
    "    #x is the input layer and y is the output layer, which label it is supposed to be\n",
    "    def back_propagation(self, x, y):\n",
    "        #pd of cost function wrt bias\n",
    "        nb = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        #pd of cost function wrt weight\n",
    "        nw = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #this is empty initially and stores the weighted sum with the bias added into it\n",
    "        z_lst = []\n",
    "        \n",
    "        #initial activation is the input layer\n",
    "        a = x\n",
    "    \n",
    "        #the list of activations, we note that the input layers activations are already set, hence why we can preset them\n",
    "        a_lst = [x]\n",
    "        \n",
    "        #computing the forward propagation\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #finding the weighted sum and storing it\n",
    "            z = w.dot(a) + b\n",
    "            z_lst.append(z)\n",
    "\n",
    "            #finding the next activation and storing it\n",
    "            a = sigmoid(z)\n",
    "            a_lst.append(a)\n",
    "            \n",
    "        \n",
    "        #error in output layer, hadamard product of pd of the cost function with the rate of change of the activation function (sigmoid)\n",
    "        delta = (a_lst[-1] - y) * (sigmoid(z_lst[-1])*(1.0-sigmoid(z_lst[-1])))\n",
    "        \n",
    "        nb[-1] = delta\n",
    "        nw[-1] = delta.dot(a_lst[-2].T)\n",
    "        \n",
    "        #backpropogate said error\n",
    "        for l in range(2, self.number_of_layers):\n",
    "            delta = ((self.weights[-l+1].T).dot(delta)) * (sigmoid(z_lst[-l])*(1-sigmoid(z_lst[-l])))\n",
    "            nb[-l] = delta\n",
    "            nw[-l] = delta.dot(a_lst[-l-1].T)\n",
    "            \n",
    "        #output\n",
    "        return (nb, nw)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d7fdbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8974 / 10000\n",
      "Epoch 1: 9096 / 10000\n",
      "Epoch 2: 9138 / 10000\n",
      "Epoch 3: 9199 / 10000\n",
      "Epoch 4: 9242 / 10000\n",
      "Epoch 5: 9191 / 10000\n",
      "Epoch 6: 9306 / 10000\n",
      "Epoch 7: 9225 / 10000\n",
      "Epoch 8: 9247 / 10000\n",
      "Epoch 9: 9320 / 10000\n",
      "Epoch 10: 9306 / 10000\n",
      "Epoch 11: 9324 / 10000\n",
      "Epoch 12: 9306 / 10000\n",
      "Epoch 13: 9324 / 10000\n",
      "Epoch 14: 9356 / 10000\n",
      "Epoch 15: 9313 / 10000\n",
      "Epoch 16: 9363 / 10000\n",
      "Epoch 17: 9291 / 10000\n",
      "Epoch 18: 9319 / 10000\n",
      "Epoch 19: 9341 / 10000\n",
      "Epoch 20: 9357 / 10000\n",
      "Epoch 21: 9390 / 10000\n",
      "Epoch 22: 9339 / 10000\n",
      "Epoch 23: 9296 / 10000\n",
      "Epoch 24: 9304 / 10000\n",
      "Epoch 25: 9395 / 10000\n",
      "Epoch 26: 9353 / 10000\n",
      "Epoch 27: 9380 / 10000\n",
      "Epoch 28: 9362 / 10000\n",
      "Epoch 29: 9404 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = network1([784,16, 16, 10])\n",
    "net.stochastic_gradient_descent(list(training), 30, 10, 3.0, test_data=list(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "70c239c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  [5]\n",
      "label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.test_predictions(1340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68ca44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
